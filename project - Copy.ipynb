{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faraaz Beyabani, Fuyao Du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from numpy import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we used the IDCAR 2013 dataset from a Kaggle competition to predict the gender of the author of a handwritten document. The dataset contains the writings of 282 unique authors, each writing 2 pages in English and 2 pages in Arabic, for a total dataset of 1128 pages of handwriting. We opted to use the pre-extracted features from the dataset, rather than extracting our own from the images. This allowed us to spend more time testing various model types, rather than building a robust feature extraction pipeline.\n",
    "\n",
    "Unfortunately we ran into some technical difficulties, as the dataset on the Kaggle website became inaccessible some time after we had individually downloaded it, meaning we had to keep backups in the event that we still needed some files.\n",
    "\n",
    "The dataset features included tortuousity, curviness, chaincode, and others. We explore the use of the first 3 below, then transition to using all pre-extracted features. Later, we also began using cross-validation with a 75/25% split, witholding 1 document from each author, resulting in 846 documents for training, and 282 for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train.csv')\n",
    "train_ans = pd.read_csv('./train_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ans = train_ans.iloc[:,1]\n",
    "y = np.repeat(train_ans.to_numpy(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent with hinge loss on tortuousity features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll test out classic stochastic gradient descent classification on the first ~40 features. These features are **tortuousity**, which describes the \"twistedness\" of the handwriting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tort_data = train_data.iloc[:, 4:44]\n",
    "\n",
    "x = train_tort_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1128, 40)\n",
      "(1128,)\n",
      "\n",
      "There are 572 male writers.\n",
      "\n",
      "There are 556 male writers.\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x))\n",
    "print(np.shape(y))\n",
    "\n",
    "print(f'\\nThere are {(y==0).sum()} male writers.')\n",
    "print(f'\\nThere are {(y==1).sum()} male writers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "model.fit(X=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = model.predict(X=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print((answers != y).sum())\n",
    "print(answers[:32])\n",
    "print(y[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get 61% accuracy (439 misclassifications over 1128 samples). We also print the first 32 predicted answers, and their corresponding ground truths. This result is alright, but we decided to try to use other features to try and achieve a better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent with hinge loss on curviness features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we attempted to use the next 855 features, describing the **curviness** of the handwriting. We wanted to test which descriptor of handwriting was accurate when it came to accurately classifying the gender of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_curve_data = train_data.iloc[:, 54:900]\n",
    "x = train_curve_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1128, 846)\n",
      "(1128,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = model.predict(X=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print((answers != y).sum())\n",
    "print(answers[:32])\n",
    "print(y[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we got very similar results to our attempt of SGD with tortuousity features. We were not optimistic about this method, but we wanted to give one other set of features a try before writing off SGD entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent with hinge loss on chaincode features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next ~4000 features are called \"chaincode\". We weren't sure what that meant, but we inferred that it referred to how letters connected to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chain_data = train_data.iloc[:, 901:5020]\n",
    "x = train_chain_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1128, 4119)\n",
      "(1128,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = model.predict(X=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "0.648936170212766\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print((answers != y).sum())\n",
    "print(model.score(X=x, y=y))\n",
    "print(answers[:32])\n",
    "print(y[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our results were a bit better than the previous two feature sets, however, there was a lot of variance every time we ran classification. Thus, we attributed this wildly different performance to SGD being a poor fit for this use case. However, we decided to continue attempting classficiation using these chaincode features, just in case the jump in accuracy was not a coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest using chaincode features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we tried to use **Random Forest** classification. We felt like this would be a good use case because of its similarity to traditional Decision Tree methods. In addition, to keep track of potential overfitting, we also implemented cross validation on the training set, using a 25/75 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chain_data = train_data.iloc[:, 901:5020]\n",
    "x = train_chain_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=250, min_samples_leaf=3)\n",
    "model.fit(X=x, y=y)\n",
    "answers = model.predict(X=x)\n",
    "print((answers != y).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, with 0 misclassifications, we must make sure that we are not overfitting with this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.arange(3, 1128, 4)\n",
    "train_idx = np.delete(np.arange(1128), test_idx)\n",
    "\n",
    "x_test = x[test_idx, :]\n",
    "x_train = x[train_idx, :]\n",
    "y_train = np.repeat(train_ans.to_numpy(), 3)\n",
    "y_test = train_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(min_samples_leaf=3, n_estimators=250)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X=x_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0\n",
      "\n",
      "\n",
      "0.7056737588652482\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X=x_train, y=y_train))\n",
    "print((model.predict(X=x_train) != y_train).sum())\n",
    "print('\\n')\n",
    "print(model.score(X=x_test, y=y_test))\n",
    "print((model.predict(X=x_test) != y_test).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This disparity in accuracy between the training accuracy and test accuracy confirms that we are overfitting, however our base accuracy of 70% is a bit better than our earlier accuracies of around 65%. We experimented with different sklearn model parameters, but unfortunately we did not gain a significant boost in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest using all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiousity, we decided to try using all of the features rather than only looking at one feature of a given type. This would give us more diversity of the data we were looking at e.g. rather than just looking at tortuousity, look at it and curviness and the chaincode, especially since random forest was decently suited for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_data = train_data.iloc[:, 4:]\n",
    "x = train_all_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=250, min_samples_leaf=1, bootstrap=False, max_features='log2',\n",
    "                              )\n",
    "model.fit(X=x, y=y)\n",
    "answers = model.predict(X=x)\n",
    "print((answers != y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.arange(3, 1128, 4)\n",
    "train_idx = np.delete(np.arange(1128), test_idx)\n",
    "\n",
    "x_test = x[test_idx, :]\n",
    "x_train = x[train_idx, :]\n",
    "y_train = np.repeat(train_ans.to_numpy(), 3)\n",
    "y_test = train_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_features='log2', n_estimators=250)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X=x_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0\n",
      "\n",
      "\n",
      "0.8014184397163121\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X=x_train, y=y_train))\n",
    "print((model.predict(X=x_train) != y_train).sum())\n",
    "print('\\n')\n",
    "print(model.score(X=x_test, y=y_test))\n",
    "print((model.predict(X=x_test) != y_test).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had decent success. Of course, we were still overfitting to the training data, but the accuracy on the validation set was, in our opinion, still quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Net with all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last attempt to obtain decent accuracy without overfitting, one of the team members used their machine learning proficiencies to develop a LSTM recurrent neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_network(train_X, train_y, test_X, test_y, layer_num):\n",
    "    np.random.seed(9)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(layer_num, activation='relu', input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=300, validation_split=0.15, verbose=2, shuffle=False)\n",
    "    # plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.show()\n",
    "    # make a prediction\n",
    "    yhat_train = model.predict(train_X)\n",
    "    yhat_train = (yhat_train > 0.5).astype(int)\n",
    "    train_XS=train_X\n",
    "    train_XS = train_XS.reshape((train_XS.shape[0], train_XS.shape[2]))\n",
    "    yhat_train = concatenate((yhat_train, train_XS[:, 1:]), axis=1)\n",
    "    yhat_train = yhat_train[:,0]\n",
    "    \n",
    "    yhat_test = model.predict(test_X)\n",
    "    yhat_test = (yhat_test > 0.5).astype(int)\n",
    "    test_X = test_X.reshape((test_X.shape[0], train_X.shape[2]))\n",
    "    yhat_test = concatenate((yhat_test, test_X[:, 1:]), axis=1)\n",
    "    \n",
    "    yhat_test = yhat_test[:,0]\n",
    "    \n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy_train = sum(yhat_train == train_y)/len(train_y)\n",
    "    accuracy_test = sum(yhat_test == test_y)/len(test_y)\n",
    "    print('Train accuracy: %.3f' % accuracy_train)\n",
    "    print('Test accuracy: %.3f' % accuracy_test )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train.csv')\n",
    "train_ans = pd.read_csv('./train_answers.csv')\n",
    "\n",
    "train_ans = train_ans.iloc[:,1]\n",
    "y = np.repeat(train_ans.to_numpy(), 4)\n",
    "All_features = (train_data.iloc[:, 5:5020]).to_numpy()\n",
    "test_idx = np.arange(3, 1128, 4)\n",
    "train_idx = np.delete(np.arange(1128), test_idx)\n",
    "\n",
    "test_X = All_features[test_idx, :]\n",
    "train_X = All_features[train_idx, :]\n",
    "train_y = np.repeat(train_ans.to_numpy(), 3)\n",
    "# Change into 3-dimension by adding time stamp so that can be used in LSTM\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "test_y = train_ans\n",
    "layer_num = 100\n",
    "\n",
    "model_LSTM = fit_network(train_X,train_y,test_X,test_y, layer_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final results for this model type were\n",
    "\n",
    "**Train accuracy: 84.5%**\n",
    "\n",
    "**Test accuracy: 81.6%**\n",
    "\n",
    "These accuracy results were very close, and even better than the ones we obtained from the Random Forest model. With this model type, we are no longer overfitting on the training data, allowing for more accurate results, overall, since our model is more generalizable to all datasets, and training now gives a more accurate representation of performance on other data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, in our opinion, we did very well in classifying the given dataset with the limited number of data and time that we had. With have 4 samples from each author, we only had the data from 282 unique authors, making our results somewhat unreliable on a larger scale.\n",
    "\n",
    "Given more time and/or resources, we believe that this problem space could benefit from an increase of data, which would be human-written texts in multiple languages. Additionally, it may also be worth trying to use the raw pixel-based image data, just to see what sort of results may occur. Furthermore, we believe extracting our own features could also be beneficial in increasing the total achievable accuracy on the data type. Finally, additional, more sophisticated model types would also be very useful in achieving better results (90%+) on this data, included hand-crafted logistic regression and/or deep-learning methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
